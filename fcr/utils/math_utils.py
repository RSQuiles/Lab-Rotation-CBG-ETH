import math
from numbers import Real
from typing import Optional, Tuple, Union

import numpy as np

import torch
import torch.nn.functional as F

def logprob_normal(x, loc, scale, weight=None, eps=1e-8):
    var = (scale ** 2)
    log_scale = math.log(scale) if isinstance(scale, Real) else scale.log()
    res = (
        -((x - loc) ** 2) / (2 * var + eps)
        - log_scale
        - math.log(math.sqrt(2 * math.pi))
    )

    if weight is not None:
        while weight.dim() < res.dim():
            weight = weight.unsqueeze(1)
        res = res * weight
    return res

def kldiv_normal(mu1: torch.Tensor, sigma1: torch.Tensor,
        mu2: torch.Tensor, sigma2: torch.Tensor) -> torch.Tensor:
    #print("SHAPES: mu1, sigma1, mu2, sigma2 --> ", mu1.shape, sigma1.shape, mu2.shape, sigma2.shape)
    ## modified: add epsilon to prevent -inf values when applying log
    eps = 1e-3
    sigma1 = sigma1.add(eps)
    sigma2 = sigma2.add(eps)
    logvar1 = 2 * sigma1.log()
    logvar2 = 2 * sigma2.log()
    result = torch.mean(-0.5 * torch.sum(1. + logvar1-logvar2 - (mu1-mu2)** 2 - (logvar1-logvar2).exp(), dim = 1), dim = 0)
    ## modified: check KL result
    if torch.isnan(result).any():
        print("logvar1: ", logvar1)
        print("logvar2: ", logvar2)
        print("sigma2: ", sigma2)
        inputs = [mu1, sigma1, mu2, sigma2]
        print("INPUTS: mu1, sigma1, mu2, sigma2")
        for index, entry in enumerate(inputs):
            if torch.isnan(entry).any():
                print("Input ", index, "contains NaN")
                print(entry.tolist())
        raise RuntimeError("Computed NaN KL Divergence!")
            
    return result

def marginalize_latent_tx(latents_mean, latents_stddev,conditions):
    cov_labels = torch.unique(conditions, dim=0)
    cond_mean = []
    cond_stddev = []
    for cov_lab in cov_labels:
        index = (conditions==cov_lab).all(dim=1)
        index = index.nonzero()
        sublatents_mean = torch.mean(latents_mean[index, :], dim=0)
        sublatents_stddev = torch.mean(latents_stddev[index, :],dim=0)
        cond_mean.append(sublatents_mean)
        cond_stddev.append(sublatents_stddev)
        
    return (cond_mean, cond_stddev)


def marginalize_latent(latents,conditions):
    cov_labels = torch.unique(conditions, dim=0)
    marginal_latent = []
    for cov_lab in cov_labels:
        index = (conditions==cov_lab).all(dim=1)
        index = index.nonzero()
        sublatents_mean = torch.mean(latents[index, :], dim=0)
        marginal_latent.append(sublatents_mean)
    marginal_latent = torch.cat(marginal_latent, dim=0)
    # print("marginal_latent shape {}".format(marginal_latent.shape))
    return marginal_latent
    
def kldiv_normal_marginal(mu1: torch.Tensor, sigma1: torch.Tensor,
        mu2: torch.Tensor, sigma2: torch.Tensor) -> torch.Tensor:
    
    mu1 = torch.mean(mu1, dim=0)
    mu2 = torch.mean(mu2, dim=0)
    sigma1 = torch.mean(sigma1, dim=0)
    sigma2 = torch.mean(sigma2, dim=0)
    logvar1 = 2 * sigma1.log()
    logvar2 = 2 * sigma2.log()
    result = torch.mean(-0.5 * torch.sum(1. + logvar1-logvar2 - (mu1-mu2)** 2 - (logvar1-logvar2).exp(), dim = 1), dim = 0)
    ## modified: check KL result
    if torch.isnan(result).any:
        print("INPUTS: mu1, sigma1, mu2, sigma2 --> ", mu1, sigma1, mu2, sigma2)
        raise RuntimeError("Computed NaN KL Divergence")
    return result

def aggregate_normal_distr(mus: list, sigmas: list):
    mus = torch.cat(mus, dim=1)
    sigmas = torch.cat(sigmas, dim=1)
    return [mus, sigmas]

def logprob_bernoulli_logits(x, logit, weight=None):
    return -F.binary_cross_entropy_with_logits(logit, x, weight=weight, reduction='none')

def logprob_zinb_positive(
    x: torch.Tensor,
    mu: torch.Tensor,
    theta: torch.Tensor,
    pi: torch.Tensor,
    weight=None,
    eps=1e-8
):
    """
    Log likelihood (scalar) of a minibatch according to a zinb model.
    Parameters
    ----------
    x
        Data
    mu
        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)
    theta
        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)
    pi
        logit of the dropout parameter (real support) (shape: minibatch x vars)
    eps
        numerical stability constant
    Notes
    -----
    We parametrize the bernoulli using the logits, hence the softplus functions appearing.
    """
    # theta is the dispersion rate. If .ndimension() == 1, it is shared for all cells (regardless of batch or labels)
    if theta.ndimension() == 1:
        theta = theta.view(
            1, theta.size(0)
        )  # In this case, we reshape theta for broadcasting

    softplus_pi = F.softplus(-pi)  #  uses log(sigmoid(x)) = -softplus(-x)
    log_theta_eps = torch.log(theta + eps)
    log_theta_mu_eps = torch.log(theta + mu + eps)
    pi_theta_log = -pi + theta * (log_theta_eps - log_theta_mu_eps)

    case_zero = F.softplus(pi_theta_log) - softplus_pi
    mul_case_zero = torch.mul((x < eps).type(torch.float32), case_zero)

    case_non_zero = (
        -softplus_pi
        + pi_theta_log
        + x * (torch.log(mu + eps) - log_theta_mu_eps)
        + torch.lgamma(x + theta)
        - torch.lgamma(theta)
        - torch.lgamma(x + 1)
    )
    mul_case_non_zero = torch.mul((x > eps).type(torch.float32), case_non_zero)

    res = mul_case_zero + mul_case_non_zero

    if weight is not None:
        while weight.dim() < res.dim():
            weight = weight.unsqueeze(1)
        res = res * weight
    return res


def logprob_nb_positive(
    x: Union[torch.Tensor, np.ndarray],
    mu: Union[torch.Tensor, np.ndarray],
    theta: Union[torch.Tensor, np.ndarray],
    weight: Union[torch.Tensor, np.ndarray] = None, 
    eps: float = 1e-8,
    log_fn: callable = torch.log,
    lgamma_fn: callable = torch.lgamma,
):
    """
    Log likelihood (scalar) of a minibatch according to a nb model.
    Parameters
    ----------
    x
        data
    mu
        mean of the negative binomial (has to be positive support) (shape: minibatch x vars)
    theta
        inverse dispersion parameter (has to be positive support) (shape: minibatch x vars)
    eps
        numerical stability constant
    """
    log = log_fn
    lgamma = lgamma_fn
    log_theta_mu_eps = log(theta + mu + eps)
    res = (
        theta * (log(theta + eps) - log_theta_mu_eps)
        + x * (log(mu + eps) - log_theta_mu_eps)
        + lgamma(x + theta)
        - lgamma(theta)
        - lgamma(x + 1)
    )

    if weight is not None:
        while weight.dim() < res.dim():
            weight = weight.unsqueeze(1)
        res = res * weight
    return res

def convert_mean_disp_to_counts_logits(mu, theta, eps=1e-6):
    """
    NB parameterizations conversion.
    Parameters
    ----------
    mu
        mean of the NB distribution.
    theta
        inverse overdispersion.
    eps
        constant used for numerical log stability. (Default value = 1e-6)
    Returns
    -------
    type
        the number of failures until the experiment is stopped
        and the success probability.
    """
    if not (mu is None) == (theta is None):
        raise ValueError(
            "If using the mu/theta NB parameterization, both parameters must be specified"
        )
    logits = (mu + eps).log() - (theta + eps).log()
    total_count = theta
    return total_count, logits

def convert_counts_logits_to_mean_disp(total_count, logits):
    """
    NB parameterizations conversion.
    Parameters
    ----------
    total_count
        Number of failures until the experiment is stopped.
    logits
        success logits.
    Returns
    -------
    type
        the mean and inverse overdispersion of the NB distribution.
    """
    theta = total_count
    mu = logits.exp() * theta
    return mu, theta
