import copy
import json

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal
import numpy as np
from .module import (
    CompoundEmbedding, MLP,
    Bernoulli, NegativeBinomial, ZeroInflatedNegativeBinomial
)
import torch.nn.utils as nn_utils

from ..utils.math_utils import (
    logprob_normal, kldiv_normal,
    kldiv_normal_marginal,
    logprob_bernoulli_logits,
    logprob_nb_positive,
    logprob_zinb_positive,
    aggregate_normal_distr,
    marginalize_latent_tx,
    marginalize_latent
    
)

import warnings
warnings.filterwarnings("ignore")

from sklearn.metrics import r2_score

#####################################################
#                     LOAD MODEL                    #
#####################################################

def load_FCR(args, state_dict=None):
    # Initialized in CPU, will be later moved to the corresponding CUDA device in the training script
    device = "cpu"

    model = FCR(
        args["num_outcomes"],
        args["num_treatments"],
        args["num_covariates"],
        embed_outcomes=args["embed_outcomes"],
        embed_treatments=args["embed_treatments"],
        embed_covariates=args["embed_covariates"],
        omega0=args["omega0"], # NLLH
        omega1=args["omega1"], # treatment_loss + covar_loss
        omega2=args["omega2"], # KL divergence
        omega3=args.get("omega3", 10.0), # Permutation loss
        omega4=args.get("omega4", 10.0), # Similarity loss
        omega5 = args.get("omega5", 10.0), # R2 loss
        dist_mode=args["dist_mode"],
        dist_outcomes=args["dist_outcomes"],
        patience=args["patience"],
        device=device,
        distance=args['distance'],
        hparams=args["hparams"],
        ## modified: added batch size and option to separate outcome embeddings
        batch_size=args["batch_size"],
        separate_outcomes_emb=args["separate_outcomes_emb"]
    )
    if state_dict is not None:
        model.load_state_dict(state_dict)

    return model

#####################################################
#                     MAIN MODEL                    #
#####################################################

class FCR(nn.Module):
    def __init__(
        self,
        num_outcomes,
        num_treatments,
        num_covariates,
        batch_size, ## modified: added argument
        embed_outcomes=True,
        embed_treatments=False,
        embed_covariates=True,
        separate_outcomes_emb=False, ## modified: added argument
        omega0=1.0,
        omega1=2.0,
        omega2=2.0,
        omega3=10.0,
        omega4=10.0,
        omega5=10.0,
        dist_mode="match",
        dist_outcomes="normal",
        type_treatments=None,
        type_covariates=None, # If None, "object", "bool" or "category", embedding is done with Compound Embedding, else, with MLP
        mc_sample_size=5,
        best_score=None,
        patience=5,
        distance="element",
        device="cuda",
        hparams="",
    ):
        super(FCR, self).__init__()
        # generic attributes
        self.num_outcomes = num_outcomes
        self.num_treatments = num_treatments
        self.num_covariates = num_covariates
        self.embed_outcomes = embed_outcomes
        self.embed_treatments = embed_treatments
        self.embed_covariates = embed_covariates
        self.dist_outcomes = dist_outcomes
        self.type_treatments = type_treatments
        self.type_covariates = type_covariates
        self.mc_sample_size = mc_sample_size
        # NEW: Use separate outcomes embeddings for each encoder
        self.separate_outcomes_emb = separate_outcomes_emb
        # fcr parameters
        self.omega0 = omega0
        self.omega1 = omega1
        self.omega2 = omega2
        # NEW: weight for permutation discriminators and similarity loss
        self.omega3 = omega3
        self.omega4 = omega4
        self.omega5 = omega5
        self.dist_mode = dist_mode

        # Early stopping
        self.best_score = best_score
        self.patience = patience
        self.patience_trials = 0
        self.distance = distance
        # set hyperparameters
        self.batch_size = batch_size
        self._set_hparams_(hparams)

        ## modified: adapt for non-sampling case
        if not self.hparams["sample_latent"]:
            print("Using NON-SAMPLING approach!")
            self.mc_sample_size = 1

        # individual-specific model
        self._init_indiv_model()

        # covariate-specific model
        self._init_covar_model()

        self.iteration = 0

        self.history = {"epoch": [], "stats_epoch": []}

        self.to_device(device)

    def _set_hparams_(self, hparams):
        """
        Set hyper-parameters to default values or values fixed by user for those
        hyper-parameters, which can be specified  as a JSON object, or a JSON string.
        """

        self.hparams = {
            "ZX_dim": 64,
            "ZT_dim":64,
            "ZXT_dim":64,            
            "outcome_emb_dim": 256,
            "treatment_emb_dim": 64,
            "covariate_emb_dim": 16,
            "encoder_width": 128,
            "encoder_depth": 3,
            "decoder_width": 128,
            "decoder_depth": 3,
            "discriminator_width": 64,
            "discriminator_depth": 2,
            "autoencoder_lr": 3e-4,
            "discriminator_lr": 3e-4,
            "autoencoder_wd": 4e-7,
            "discriminator_wd": 4e-7,
            "discriminator_steps": 3,
            "step_size_lr": 45,
            "sample_latent": True
        }

        if hparams != "":
            if isinstance(hparams, str):
                with open(hparams) as f:
                    dictionary = json.load(f)
                self.hparams.update(dictionary)
            else:
                self.hparams.update(hparams)

        ## modified: compute latent_exp_dim dynamically
        self.hparams["latent_exp_dim"] = self.hparams["ZX_dim"] + self.hparams["ZT_dim"] + self.hparams["ZXT_dim"]

        self.outcome_dim = (
            self.hparams["outcome_emb_dim"] if self.embed_outcomes else self.num_outcomes)
        self.treatment_dim = (
            self.hparams["treatment_emb_dim"] if self.embed_treatments else self.num_treatments)
        self.covariate_dim = (
            self.hparams["covariate_emb_dim"]*len(self.num_covariates) 
            if self.embed_covariates else sum(self.num_covariates)
        )
        self.treatment_mixed_dim = self.covariate_dim

        return self.hparams

    def _init_indiv_model(self):
        self.params_autoencoder = []

        # embeddings
        if self.embed_outcomes:
            # Separate outcome embeddings for each encoder to prevent information leakage
            if self.separate_outcomes_emb:
                self.outcomes_embeddings_ZX = self.init_outcome_emb()
                self.outcomes_embeddings_ZT = self.init_outcome_emb()
                self.outcomes_embeddings_ZXT = self.init_outcome_emb()
                self.params_autoencoder.extend(list(self.outcomes_embeddings_ZX.parameters()))
                self.params_autoencoder.extend(list(self.outcomes_embeddings_ZT.parameters()))
                self.params_autoencoder.extend(list(self.outcomes_embeddings_ZXT.parameters()))
            # Unique outcome embedding for all encoders
            else:
                self.outcomes_embeddings = self.init_outcome_emb()
                self.params_autoencoder.extend(list(self.outcomes_embeddings.parameters()))

            self.outcomes_contr_embeddings = self.init_outcome_emb()
            self.params_autoencoder.extend(list(self.outcomes_contr_embeddings.parameters()))

        if self.embed_treatments:
            self.treatments_embeddings = self.init_treatment_emb()
            self.params_autoencoder.extend(list(self.treatments_embeddings.parameters()))

        if self.embed_covariates:
            self.covariates_embeddings = nn.Sequential(*self.init_covariates_emb())
            for emb in self.covariates_embeddings:
                self.params_autoencoder.extend(list(emb.parameters()))
                
        self.treatments_mixed_embeddings = self.init_treatment_mixed_emb()
        self.params_autoencoder.extend(list(self.treatments_mixed_embeddings.parameters()))

        ## dimension parameters
        self.ZX_dim = self.hparams["ZX_dim"]
        self.ZT_dim = self.hparams["ZT_dim"]
        self.ZXT_dim = self.hparams["ZXT_dim"]
        
        # MODIFIED: avoid unused parameters for a smooth implementation with DDP
        # models
        ## exp_encoder and control_encoder
        # self.exp_encoder = self.init_encoder_exp()
        self.encoder_ZX = self.init_encoder_X()
        self.encoder_ZT = self.init_encoder_T()
        self.encoder_ZXT = self.init_encoder_XT()

        
        ## control encoder brings in control-specific latent path
        # self.control_encoder = self.init_encoder_control()
        self.params_autoencoder.extend(list(self.encoder_ZX.parameters()))
        self.params_autoencoder.extend(list(self.encoder_ZT.parameters()))
        self.params_autoencoder.extend(list(self.encoder_ZXT.parameters()))
        # self.params_autoencoder.extend(list(self.control_encoder.parameters()))

        ## initialize the prior encoders
        self.encoder_ZX_prior = self.init_encoder_X_prior()
        self.encoder_ZT_prior = self.init_encoder_T_prior()
        self.encoder_ZXT_prior = self.init_encoder_XT_prior()
        # self.control_prior = self.init_control_prior()
        self.params_autoencoder.extend(list(self.encoder_ZX_prior.parameters()))
        self.params_autoencoder.extend(list(self.encoder_ZT_prior.parameters()))
        self.params_autoencoder.extend(list(self.encoder_ZXT_prior.parameters()))
        # self.params_autoencoder.extend(list(self.control_prior.parameters()))

        # Decoders
        self.decoder = self.init_decoder_experiments()
        self.params_autoencoder.extend(list(self.decoder.parameters()))
        self.control_decoder = self.init_decoder_control()
        self.params_autoencoder.extend(list(self.control_decoder.parameters()))

        # Covariate decoder
        self.cov_decoder = self.init_decoder_cov()
        self.params_autoencoder.extend(list(self.cov_decoder.parameters()))

        # Intervention decoder
        # print("intervention decoder style {}".format(self.distance))
        if self.distance == "cosine":
            self.interv_decoder = self.init_decoder_interv()
        elif self.distance == "element": 
            self.interv_decoder = self.init_decoder_interv_element()
        elif self.distance == "concat":
            self.interv_decoder = self.init_decoder_interv_concat()
        elif self.distance=="single":
            self.interv_decoder = self.init_decoder_interv_single()
        self.params_autoencoder.extend(list(self.interv_decoder.parameters()))

        return self.decoder, self.cov_decoder, self.interv_decoder

    def _init_covar_model(self):

        # if self.dist_mode == "discriminate":
        self.params_discriminator = []

        # embeddings
        if self.embed_outcomes:
            self.adv_outcomes_emb = self.init_outcome_emb()
            self.params_discriminator.extend(list(self.adv_outcomes_emb.parameters()))

        if self.embed_treatments:
            self.adv_treatments_emb = self.init_treatment_emb()
            self.params_discriminator.extend(list(self.adv_treatments_emb.parameters()))

        if self.embed_covariates:
            self.adv_covariates_emb = nn.Sequential(*self.init_covariates_emb())
            for emb in self.adv_covariates_emb:
                self.params_discriminator.extend(list(emb.parameters()))

        # model
        self.discriminator_X = self.init_discriminator_X()
        self.loss_discriminator_X = nn.BCEWithLogitsLoss()
        self.params_discriminator.extend(list(self.discriminator_X.parameters()))

        self.discriminator_T = self.init_discriminator_T()
        self.loss_discriminator_T = nn.BCEWithLogitsLoss()
        self.params_discriminator.extend(list(self.discriminator_T.parameters()))
        # NEW: define a generic BCE loss to avoid AttributeError in legacy code
        self.loss_discriminator = nn.BCEWithLogitsLoss()
        # print("initialized discriminator {}".format(self.discriminator_T))

        return self.discriminator_X, self.discriminator_T

#         elif self.dist_mode == "fit":
#             raise NotImplementedError(
#                 'TODO: implement dist_mode "fit" for distribution loss')

#         elif self.dist_mode == "match":
#             return None

#         else:
#             raise ValueError("dist_mode not recognized")

    def encode_control(self, outcomes, covariates, eval=False):
        
        if self.embed_outcomes:
            outcomes = self.outcomes_embeddings(outcomes)
            
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]

        covariates = torch.cat(covariates, -1)
        inputs = torch.cat([outcomes, covariates], -1)

        if eval:
            return self.control_encoder_eval(inputs)
        else:
            return self.control_encoder(inputs)
        
    def encode_exp(self, outcomes, covariates, treatments, eval=False):
        
        if self.embed_outcomes:
            outcomes = self.outcomes_embeddings(outcomes)
        
        if self.embed_treatments:
            treatments = self.treatments_embeddings(treatments) 
            
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]

        covariates = torch.cat(covariates, -1)
        inputs = torch.cat([outcomes,treatments ,covariates], -1)

        if eval:
            return self.exp_encoder_eval(inputs)
        else:
            return self.exp_encoder(inputs)
        
        
    def encode_ZX(self, outcomes, covariates, eval=False):
        """
        Inputs:
        - covariates: Python list, one tensor of shape (batch_size = B,) per covariate
        - self.covariates_embeddings: list of Compound Embeddings, one per covariate

        After the embedding:
        - emb(covars) returns shape (B, emb_dim), for each covariate
        - Thus, concatenating along the last dimension of covariates returns shape (B, sum_embedding_dimension), where
        sum_embedding_dimension = embd_dim_0 + emb_dim_1 + ...

        This is then concatenated with the outcomes, and inputted to the encoder, with an adequate shape.
        """
        
        if self.embed_outcomes:
            # Use ZX-specific outcome embedding to prevent information leakage
            if self.separate_outcomes_emb:
                outcomes = self.outcomes_embeddings_ZX(outcomes)
            else:
                outcomes = self.outcomes_embeddings(outcomes)

        if self.embed_covariates:
            # print("covariates[0] {}".format(covariates[0]))
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]
        covariates = torch.cat(covariates, -1)

        inputs = torch.cat([outcomes, covariates], -1)

        if eval:
            return self.encoder_ZX_eval(inputs)
        else:
            return self.encoder_ZX(inputs)
        
    def encode_ZT(self, outcomes, treatments, eval=False):
        if self.embed_outcomes:
            # Use ZT-specific outcome embedding to prevent information leakage
            if self.separate_outcomes_emb:
                outcomes = self.outcomes_embeddings_ZT(outcomes)
            else:
                outcomes = self.outcomes_embeddings(outcomes)

        if self.embed_treatments:
            treatments = self.treatments_embeddings(treatments)
            
        inputs = torch.cat([outcomes, treatments], -1)

        if eval:
            return self.encoder_ZT_eval(inputs)
        else:
            return self.encoder_ZT(inputs)
        
    
    def encode_ZXT(self, outcomes, covariates, treatments, eval=False):
        ## modified: added the outcome embeddings
        if self.embed_outcomes:
            # Use ZXT-specific outcome embedding to prevent information leakage
            if self.separate_outcomes_emb:
                outcomes = self.outcomes_embeddings_ZXT(outcomes)
            else:
                outcomes = self.outcomes_embeddings(outcomes)
    
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]
            
        covariates = torch.cat(covariates, -1)
        treatments = self.treatments_mixed_embeddings(treatments)

        ## modified: added the treatments to the inputs, which should make sense
        inputs = torch.cat([outcomes, covariates, treatments], -1)

        if eval:
            return self.encoder_ZXT_eval(inputs)
        else:
            return self.encoder_ZXT(inputs)
            

          
    def encode_ZT_prior(self, treatments, eval=False):
        if self.embed_treatments:
            treatments = self.treatments_embeddings(treatments)
    
        inputs = treatments

        if eval:
            return self.encoder_ZT_prior_eval(inputs)
        else:
            return self.encoder_ZT_prior(inputs)    
        
    
    def encode_ZX_prior(self, covariates, eval=False):
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]
        covariates = torch.cat(covariates, -1)
        
        inputs = covariates

        if eval:
            return self.encoder_ZX_prior_eval(inputs)
        else:
            return self.encoder_ZX_prior(inputs)
        
        
        
    def encode_ZXT_prior(self, covariates, treatments, eval=False):
        
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]
            
        covariates = torch.cat(covariates, -1)
        treatments = self.treatments_mixed_embeddings(treatments)
        inputs = torch.mul(covariates, treatments)

        if eval:
            return self.encoder_ZXT_prior_eval(inputs)
        else:
            return self.encoder_ZXT_prior(inputs)        
     
    
    def encode_control_prior(self, covariates, eval=False):
        
        if self.embed_covariates:
            covariates = [emb(covars) for covars, emb in 
                zip(covariates, self.covariates_embeddings)
            ]
        covariates = torch.cat(covariates, -1)
        inputs = covariates

        if eval:
            return self.control_prior_eval(inputs)
        else:
            return self.control_prior(inputs)    
        
    
    def decode(self, latents, eval=False):
        inputs = latents
        if eval:
            return self.decoder_eval(inputs)
        else:
            return self.decoder(inputs)
    

    def control_decode(self, latents):
        
        inputs = latents
        return self.control_decoder(inputs)
    
    def covariate_decode(self, ZX):        
        inputs = ZX
        return self.cov_decoder(inputs)
    
    
    def intervention_decode(self, ZT, latent_control):
        if self.distance == "cosine":
            similarities = torch.nn.CosineSimilarity(dim=1)
            inputs = similarities(ZT, latent_control)
            inputs = torch.unsqueeze(inputs, 1)
        elif self.distance=="element":
            inputs = torch.sub(ZT, latent_control)
        elif self.distance == "concat":
            inputs = torch.cat([ZT ,latent_control], -1)
    
        return self.interv_decoder(inputs)
        
    
    def discriminate_T(self, simulation):

        inputs = simulation

        return self.discriminator_T(inputs).squeeze()
    
    def discriminate_X(self, simulation):

        inputs = simulation

        return self.discriminator_X(inputs).squeeze()
    

    def reparameterize(self, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:
        """
        Reparameterization trick to sample from N(mu, var) from
        N(0,1).
        :param mu: (Tensor) Mean of the latent Gaussian [B x D]
        :param sigma: (Tensor) Standard deviation of the latent Gaussian [B x D]
        :return: (Tensor) [B x D]
        """
        eps = torch.randn_like(sigma)
        return eps * sigma + mu

    def distributionize(self, constructions, dim=None, dist=None, eps=1e-3):
        if dim is None:
            dim = self.num_outcomes
        if dist is None:
            dist = self.dist_outcomes

        if dist == "nb":
            mus = F.softplus(constructions[..., 0]).add(eps)
            thetas = F.softplus(constructions[..., 1]).add(eps)
            dist = NegativeBinomial(
                mu=mus, theta=thetas
            )
        elif dist == "zinb":
            mus = F.softplus(constructions[..., 0]).add(eps)
            thetas = F.softplus(constructions[..., 1]).add(eps)
            zi_logits = constructions[..., 2].add(eps)
            dist = ZeroInflatedNegativeBinomial(
                mu=mus, theta=thetas, zi_logits=zi_logits
            )
        elif dist == "normal":
            locs = constructions[..., 0]
            scales = F.softplus(constructions[..., 1]).add(eps)
            # BUG: stray 'scales' token removed
            dist = Normal(
                loc=locs, scale=scales
            )
        elif dist == "bernoulli":
            logits = constructions[..., 0]
            dist = Bernoulli(
                logits=logits
            )

        return dist

    # BUG: deprecated sample() (mismatched decode signature) disabled; use sample_expr()
    def sample_latent(self,  mu: torch.Tensor, sigma: torch.Tensor, size=1):
        
        mu = mu.repeat(size, 1)
        sigma = sigma.repeat(size, 1)
        latents = self.reparameterize(mu, sigma)
        return latents
    
    def sample_control(self, mu: torch.Tensor, sigma: torch.Tensor,
            size=1) -> torch.Tensor:
        
        mu = mu.repeat(size, 1)
        sigma = sigma.repeat(size, 1)
        latents = self.reparameterize(mu, sigma)
        return self.control_decode(latents)

    ## modified: new function to forward the latent mean directly
    def forward_control(self, mu: torch.Tensor):
        return self.control_decode(mu)
    
    def sample_expr(self, mu: torch.Tensor, sigma: torch.Tensor,size=1, eval: bool = False) -> torch.Tensor:
        
        mu = mu.repeat(size, 1)
        sigma = sigma.repeat(size, 1)
        latents = self.reparameterize(mu, sigma)        
        return self.decode(latents, eval=eval)

    ## modified: new function to forward the latent mean directly
    def forward_expr(self, mu: torch.Tensor):
        return self.decode(mu)
    
    ## permutation function for latent space Z_XT
    ## input: joint distribution mu and sigma, and ZXT's sigma mu
    ## output: joint sampled ZXT and ZT, and independently sampled ZXT with previous sampled ZT
    
    def permutation_distribution_X(self, mu, sigma, conditions):
        sample1 = self.sample_latent(mu, sigma, size=1)
        # Returns list of average latent means for each unique condition:
        sample1 = marginalize_latent(sample1, conditions)
        ZX1 = sample1[:, :self.ZX_dim] # subsets to ZX part of the latent
        ZXT1 = sample1[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label1 = torch.ones(sample1.shape[0], device=self.device, dtype=torch.float32).unsqueeze(1)
        original = torch.cat([ZX1, ZXT1, label1], -1)
        
        
        ## resample the ZXT
        sample2 = self.sample_latent(mu, sigma, size=1)
        sample2 = marginalize_latent(sample2, conditions)
        ZXT2 = sample2[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label2 = torch.zeros(sample2.shape[0],device=self.device, dtype=torch.float32).unsqueeze(1)
        perturbed = torch.cat([ZX1, ZXT2,label2], -1)
        # print("orignal shape {}, perturb shape {}".format(original.shape, perturbed.shape))
        
        combined =  torch.cat([original, perturbed], 0)
        index = torch.randperm(combined.shape[0])
        combined = combined[index, :]
        # print("combined shape {}".format(combined.shape))
        
        return combined
        
        
    def permutation_distribution_T(self, mu, sigma, conditions):
        sample1 = self.sample_latent(mu, sigma, size=1)
        sample1 = marginalize_latent(sample1, conditions)
        ZT1 = sample1[:, self.ZX_dim+self.ZXT_dim:]
        ZXT1 = sample1[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label1 = torch.ones(sample1.shape[0], dtype=torch.float32,device=self.device).unsqueeze(1)
        original = torch.cat([ZXT1,ZT1,label1], -1)
        
        ## resample the ZXT
        sample2 = self.sample_latent(mu, sigma, size=1)
        sample2 = marginalize_latent(sample2, conditions)
        ZXT2 = sample2[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]  # BUGFIX: was taking ZT slice
        label2 = torch.zeros(sample2.shape[0], dtype=torch.float32,device=self.device).unsqueeze(1)
        perturbed = torch.cat([ZXT2,ZT1,label2], -1)
        # print("orignal shape {}, perturb shape {}".format(original.shape, perturbed.shape))
        
        ##
        combined =  torch.cat([original, perturbed], 0)
        index = torch.randperm(combined.shape[0])
        combined = combined[index, :]
        # print("combined shape {}".format(combined.shape))
        
        return combined
    
    
    
    def permutation_samples_X(self, mu: torch.Tensor, sigma: torch.Tensor, size=1):
        
        sample1 = self.sample_latent(mu, sigma, size)
        ZX1 = sample1[:, :self.ZX_dim]
        ZXT1 = sample1[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label1 = torch.ones(sample1.shape[-1], dtype=torch.float64)
        original = torch.cat([ZX1, ZXT1, label1], -1)
        
        ## resample the ZXT
        sample2 = self.sample_latent(mu, sigma, size)
        ZXT2 = sample2[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label2 = torch.zeros(sample2.shape[-1], dtype=torch.float64)
        perturbed = torch.cat([ZX1, ZXT2,label2], -1)
        print("orignal shape {}, perturb shape {}".format(original.shape, perturbed.shape))
        
        ##
        combined =  torch.cat([original, perturbed], 0)
        index = torch.randperm(combined.shape[0])
        combined = combined[index, :]
        print("combined shape {}".format(combined.shape))
        
        return combined
        
        
    def permutation_samples_T(self, mu: torch.Tensor, sigma: torch.Tensor, size=1):
        
        sample1 = self.sample_latent(mu, sigma, size)
        ZT1 = sample1[:, self.ZX_dim+self.ZXT_dim:]
        ZXT1 = sample1[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]
        label1 = torch.ones(sample1.shape[-1], dtype=torch.float64)
        original = torch.cat([ZXT1,ZT1,label1], -1)
        
        ## resample the ZXT
        sample2 = self.sample_latent(mu, sigma, size)
        ZXT2 = sample2[:, self.ZX_dim:self.ZX_dim+self.ZXT_dim]  # BUGFIX: was taking ZT slice
        label2 = torch.zeros(sample2.shape[-1], dtype=torch.float64)
        perturbed = torch.cat([ZXT2,ZT1,label2], -1)
        print("orignal shape {}, perturb shape {}".format(original.shape, perturbed.shape))
        
        ##
        combined =  torch.cat([original, perturbed], 0)
        index = torch.randperm(combined.shape[0])
        combined = combined[index, :]
        print("combined shape {}".format(combined.shape))
        
        return combined
    

    def predict(
        self,
        outcomes,
        treatments,
        cf_treatments,
        covariates,
        return_dist=False
    ):
        outcomes, treatments, cf_treatments, covariates = self.move_inputs(
            outcomes, treatments, cf_treatments, covariates
        )
        if cf_treatments is None:
            cf_treatments = treatments

        with torch.autograd.no_grad():
            latents_constr = self.encode_exp(outcomes, covariates, treatments)
            latents_dist = self.distributionize(
                latents_constr, dim=self.hparams["latent_exp_dim"], dist="normal"
            )

            outcomes_constr = self.decode(latents_dist.mean)
            outcomes_dist = self.distributionize(outcomes_constr)

        if return_dist:
            return outcomes_dist
        else:
            return outcomes_dist.mean
        
        
    def predict_self(
        self,
        outcomes,
        treatments,
        #cf_treatments, ## modified: cf_treatments are not used in the function
        covariates,
        eval = False,
        return_dist=False,
        control = False ## modified: defines which decoder to use
    ):
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        with torch.autograd.no_grad():
            ZX_constr = self.encode_ZX(outcomes, covariates, eval=eval)
            ZX_dist = self.distributionize(
            ZX_constr, dim=self.hparams["ZX_dim"], dist="normal"
        )
        
            ZT_constr = self.encode_ZT(outcomes, treatments, eval=eval)
            ZT_dist = self.distributionize(
                ZT_constr, dim=self.hparams["ZT_dim"], dist="normal"
            )

            ## modified: added ZXT
            ZXT_constr = self.encode_ZXT(outcomes, covariates, treatments, eval=eval)
            ZXT_dist = self.distributionize(
                ZXT_constr, dim=self.hparams["ZXT_dim"], dist="normal"
            )

            ## modified: sample_expr needs one tensor for the mean, one tensor for the stddev
            ##           and applies a decoder with input dimension latent_exp_dim
            latents_dist_mean = torch.cat([ZX_dist.mean, ZXT_dist.mean, ZT_dist.mean], dim = 1)
            latents_dist_stddev = torch.cat([ZX_dist.stddev, ZXT_dist.stddev, ZT_dist.stddev], dim = 1)
            #latents_dist = self.distributionize(torch.stack([latents_dist_mean, latents_dist_stddev], dim=-1))
            if not control:
                outcomes_constr = self.sample_expr(latents_dist_mean, latents_dist_stddev, eval=eval)
            else:
                outcomes_constr = self.sample_control(latents_dist_mean, latents_dist_stddev)
            outcomes_dist = self.distributionize(outcomes_constr)
 

        if return_dist:
            return outcomes_dist
        else:
            return outcomes_dist.mean

    def predict_self_eval(
        self,
        outcomes,
        treatments,
        #cf_treatments, ## modified: cf_treatments are not used in the function
        covariates,
        return_dist=False
    ):
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        with torch.autograd.no_grad():
            ZX_constr = self.encode_ZX(outcomes, covariates)
            ZX_dist = self.distributionize(
            ZX_constr, dim=self.hparams["ZX_dim"], dist="normal"
        )
        
            ZT_constr = self.encode_ZT(outcomes, treatments)
            ZT_dist = self.distributionize(
                ZT_constr, dim=self.hparams["ZT_dim"], dist="normal"
            )

            ## modified: added ZXT
            ZXT_constr = self.encode_ZXT(outcomes, covariates, treatments)
            ZXT_dist = self.distributionize(
                ZXT_constr, dim=self.hparams["ZXT_dim"], dist="normal"
            )

            ## modified: sample_expr needs one tensor for the mean, one tensor for the stddev
            ##           and applies a decoder with input dimension latent_exp_dim
            latents_dist_mean = torch.cat([ZX_dist.mean, ZXT_dist.mean, ZT_dist.mean], dim = 1)
            latents_dist_stddev = torch.cat([ZX_dist.stddev, ZXT_dist.stddev, ZT_dist.stddev], dim = 1)
            #latents_dist = self.distributionize(torch.stack([latents_dist_mean, latents_dist_stddev], dim=-1))
            outcomes_constr = self.sample_expr(latents_dist_mean, latents_dist_stddev)
            outcomes_dist = self.distributionize(outcomes_constr)
 

        if return_dist:
            return outcomes_dist
        else:
            return outcomes_dist.mean

    # BUG: generate() relied on undefined self.encode and a mismatched sample(); disabled for safety
    # def generate(...):
    #     pass

    def logprob(self, outcomes, outcomes_param, dist=None):
        """
        Compute log likelihood.
        """
        if dist is None:
            dist = self.dist_outcomes

        num = len(outcomes)
        if isinstance(outcomes, list):
            sizes = torch.tensor(
                [out.size(0) for out in outcomes], device=self.device
            )
            weights = torch.repeat_interleave(1./sizes, sizes, dim=0)
            outcomes_param = [
                torch.repeat_interleave(out, sizes, dim=0) 
                for out in outcomes_param
            ]
            outcomes = torch.cat(outcomes, 0)
        elif isinstance(outcomes_param[0], list):
            sizes = torch.tensor(
                [out.size(0) for out in outcomes_param[0]], device=self.device
            )
            weights = torch.repeat_interleave(1./sizes, sizes, dim=0)
            outcomes = torch.repeat_interleave(outcomes, sizes, dim=0)
            outcomes_param = [
                torch.cat(out, 0)
                for out in outcomes_param
            ]
        else:
            weights = None

        if dist == "nb":
            logprob = logprob_nb_positive(outcomes,
                mu=outcomes_param[0],
                theta=outcomes_param[1],
                weight=weights
            )
        elif dist == "zinb":
            logprob = logprob_zinb_positive(outcomes,
                mu=outcomes_param[0],
                theta=outcomes_param[1],
                zi_logits=outcomes_param[2],
                weight=weights
            )
        elif dist == "normal":
            logprob = logprob_normal(outcomes,
                loc=outcomes_param[0],
                scale=outcomes_param[1],
                weight=weights
            )
        elif dist == "bernoulli":
            logprob = logprob_bernoulli_logits(outcomes,
                loc=outcomes_param[0],
                weight=weights
            )

        return (logprob.sum(0)/num).mean()

    ## LEGACY CODE: not used in current implementation
    def loss(self, outcomes, outcomes_dist_samp,
            cf_outcomes, cf_outcomes_out,
            latents_dist, cf_latents_dist,
            treatments, cf_treatments,
            covariates, kde_kernel_std=1.0):
        """
        Compute losses.
        """
        # (1) individual-specific likelihood
        indiv_spec_nllh = -outcomes_dist_samp.log_prob(
            outcomes.repeat(self.mc_sample_size, *[1]*(outcomes.dim()-1))
        ).mean()

        # (2) covariate-specific likelihood
        if self.dist_mode == "discriminate":
            if self.iteration % self.hparams["discriminator_steps"]:
                self.update_discriminator(
                    outcomes, cf_outcomes_out.detach(),
                    treatments, cf_treatments, covariates
                )

            # BUG: legacy path expected self.discriminate() and self.loss_discriminator (not defined). 
            # covar_spec_nllh = self.loss_discriminator(
            #     self.discriminate(cf_outcomes_out, cf_treatments, covariates),
            #     torch.ones(cf_outcomes_out.size(0), device=cf_outcomes_out.device)
            # )
            covar_spec_nllh = torch.tensor(0.0, device=cf_outcomes_out.device)  # keep placeholder so code runs
        elif self.dist_mode == "fit":
            raise NotImplementedError(
                'TODO: implement dist_mode "fit" for distribution loss')
        elif self.dist_mode == "match":
            notNone = [o != None for o in cf_outcomes]
            cf_outcomes = [o for (o, n) in zip(cf_outcomes, notNone) if n]
            cf_outcomes_out = cf_outcomes_out[notNone]

            kernel_std = [kde_kernel_std * torch.ones_like(o) 
                for o in cf_outcomes]
            covar_spec_nllh = -self.logprob(
                cf_outcomes_out, (cf_outcomes, kernel_std), dist="normal"
            )

        # (3) kl divergence
        kl_divergence = kldiv_normal(
            latents_dist.mean,
            latents_dist.stddev,
            cf_latents_dist.mean,
            cf_latents_dist.stddev
        )

        return (indiv_spec_nllh, covar_spec_nllh, kl_divergence)
    

    def loss_paired(self, control_outcomes, control_outcomes_dist_samp,
            expr_outcomes, expr_outcomes_dist_samp, exp_dist, ZX_dist, ZT_dist, ZXT_dist,
            ZX_prior_dist, ZT_prior_dist, ZXT_prior_dist, control_prior_dist,control_latents_dist,
            cov_constr, treatment_constr, treatments, covariates, 
            kde_kernel_std=1.0, single_treatment=False):
        """
        Compute losses.
        """
        # indiv_spec_nllh = -outcomes_dist_samp.log_prob(
        #     outcomes.repeat(self.mc_sample_size, *[1]*(outcomes.dim()-1))
        # ).mean()

        """
        # Check devices:
        print(f"Control outcomes device: {control_outcomes.device}")
        print(f"Control outcomes dist sample device: {control_outcomes_dist_samp.device}")
        print(f"Expr outcomes device: {expr_outcomes.device}")
        print(f"Expr outcomes dist sample device: {expr_outcomes_dist_samp.device}")
        print(f"Exp dist device: {exp_dist.device}")
        print(f"ZX dist device: {ZX_dist.device}")
        print(f"ZT dist device: {ZT_dist.device}")
        print(f"ZXT dist device: {ZXT_dist.device}")
        print(f"ZX prior dist device: {ZX_prior_dist.device}")
        print(f"ZT prior dist device: {ZT_prior_dist.device}")
        print(f"ZXT prior dist device: {ZXT_prior_dist.device}")
        print(f"Control prior dist device: {control_prior_dist.device}")
        print(f"Control latents dist device: {control_latents_dist.device}")
        print(f"Covariates device: {covariates.device}")
        print(f"Treatments device: {treatments.device}")
        """

        # 1) Likelihood for control outcomes
        indiv_spec_nllh_control = -control_outcomes_dist_samp.log_prob(
            control_outcomes.repeat(self.mc_sample_size, *[1]*(control_outcomes.dim()-1))
        ).mean()
            
        # 2) Likelihood for treated outcomes

        # print("expr_outcomes_dist_samp mean shape is {}".format(expr_outcomes_dist_samp.mean.shape))
        # print("expr_outcomes shape is {}".format(expr_outcomes.shape))
        indiv_spec_nllh_experiments = -expr_outcomes_dist_samp.log_prob(
            expr_outcomes.repeat(self.mc_sample_size, *[1]*(expr_outcomes.dim()-1))
        ).mean()
            
        # 3) Reconstruction loss of covariates from ZX 
        cross_entropy_loss = nn.CrossEntropyLoss()
        cov_loss = 0.0
        # print(f"cov_constr.shape: {cov_constr.shape}")
        # Note covariates should be a list of tensors, one per covariate
        for i in range(len(covariates)):
            # print(f"len(covariates): {len(covariates)}")
            # print(f"cov_constr[...,i].shape: {cov_constr[...,i].shape}")
            # print(f"covariates[i].shape: {covariates[i].shape}")
            cov_loss = cov_loss + cross_entropy_loss(cov_constr[...,i],covariates[i].squeeze())

        # 4) Reconstruction loss of treatments from (ZTs, ZTs_control)
        if not single_treatment:
            treatment_loss = cross_entropy_loss(treatment_constr.squeeze(),torch.argmax(treatments, 1))
        else:
            treatment_loss = torch.tensor(0.0, device=self.device) # placeholder if only one treatment (no treatment reconstruction loss)

        # KL divergences:

        agg_prior = aggregate_normal_distr([ZX_prior_dist.mean, ZXT_prior_dist.mean, ZT_prior_dist.mean],\
                                           [ZX_prior_dist.stddev, ZXT_prior_dist.stddev, ZT_prior_dist.stddev] )
        
        # 5) KL divergence between treated posterior and prior
        kl_divergence_ind =  kldiv_normal(
            agg_prior[0],
            agg_prior[1],
            exp_dist.mean,
            exp_dist.stddev,
        )
        
        ## 6) KL divergence for marginalized distributions
        """
        This code is computing a group-wise (marginalized) KL divergence over conditions (covariate + treatment)
        It enforces the model to align the condition-level posterior with the condition-level prior, 
        i.e. make latent representations for a given condition coherent.
        """
        ## ZX divergence
        covariates = torch.cat(covariates, dim=1) # Turn list of tensors into tensor, where each row is the covariates of the sample
        conditions = torch.cat((treatments, covariates), dim=1) # Covariate - Treatment (condition) combination for each sample
        conditions_labels = torch.unique(conditions, dim=0)

        """
        marginalize_latent_tx returns a tuple:
        - cond_mean: average mean vector (for latent ZX) for each unique condition
        - cond_stddev: average stddev vector (for latent ZX) for each unique condition
        """
        marginal_ZX_prior =  marginalize_latent_tx(ZX_prior_dist.mean, ZX_prior_dist.stddev, conditions)
        ## modified: index as ZX_dist[...,0/1] to get last dimension, instead of ZX_dist[0/1]
        #print("ZX_dist: ", ZX_dist.shape)
        marginal_ZX = marginalize_latent_tx(ZX_dist[...,0], ZX_dist[...,1], conditions)
        kl_divergence_X = torch.tensor(0, dtype=torch.float64, device=self.device)

        # Sum over KL divergences for all conditions
        for i in range(conditions_labels.shape[0]):
            kl_divergence_X +=kldiv_normal(
            marginal_ZX_prior[0][i],
            marginal_ZX_prior[1][i],
            marginal_ZX[0][i],
            marginal_ZX[1][i]        
            )
        
      
        ## ZT divergence
        # We apply the same rationale as above, but for ZT latent
        marginal_ZT_prior =  marginalize_latent_tx(ZT_prior_dist.mean, ZT_prior_dist.stddev, conditions)
        ## modified: index as ZX_dist[...,0/1] to get last dimension, instead of ZX_dist[0/1]
        marginal_ZT = marginalize_latent_tx(ZT_dist[...,0], ZT_dist[...,1], conditions)
        kl_divergence_T = torch.tensor(0, dtype=torch.float64, device=self.device)
        for i in range(conditions_labels.shape[0]):
            kl_divergence_T +=kldiv_normal(
            marginal_ZT_prior[0][i],
            marginal_ZT_prior[1][i],
            marginal_ZT[0][i],
            marginal_ZT[1][i]        
            )
        
        ## Z_XT divergence
        # We apply the same rationale as above, but for ZXT latent
        marginal_ZXT_prior =  marginalize_latent_tx(ZXT_prior_dist.mean, ZXT_prior_dist.stddev, conditions)
        ## modified: index as ZX_dist[...,0/1] to get last dimension, instead of ZX_dist[0/1]
        marginal_ZXT = marginalize_latent_tx(ZXT_dist[...,0], ZXT_dist[...,1], conditions)
        kl_divergence_XT = torch.tensor(0,dtype=torch.float64, device=self.device)
        for i in range(conditions_labels.shape[0]):
            kl_divergence_XT +=kldiv_normal(
            marginal_ZXT_prior[0][i],
            marginal_ZXT_prior[1][i],
            marginal_ZXT[0][i],
            marginal_ZXT[1][i]        
            )
        
        # 7) KL divergence between control posterior and prior
        kl_divergence_control = kldiv_normal(
            control_prior_dist[0],
            control_prior_dist[1],
            control_latents_dist.mean,
            control_latents_dist.stddev
        )

        return (indiv_spec_nllh_control, indiv_spec_nllh_experiments, cov_loss, treatment_loss,
                kl_divergence_ind, kl_divergence_X, kl_divergence_T, kl_divergence_XT,kl_divergence_control, conditions, conditions_labels)


    def forward(self, outcomes, treatments, control_outcomes, covariates,
                adv_training=False, sample_latent=True, detach_encode=False, detach_eval=True,
                single_treatment=False # Eliminates treatment reconstruction loss if only one treatment (e.g., control) is used
                ):
        """
        Execute the workflow.
        """
        # Move inputs to device
        # print(f"Model device: {self.device}")
        outcomes, treatments, control_outcomes, covariates = self.move_inputs(
            outcomes, treatments, control_outcomes, covariates
        )

        # POSTERIORS:
        # q(zx_control | y_control, x)
        control_treatment = torch.zeros(treatments.shape, dtype=torch.float32, device=treatments.device)
        ZX_control_constr = self.encode_ZX(control_outcomes, covariates)
        ZX_control_dist = self.distributionize(
            ZX_control_constr, dim=self.hparams["ZX_dim"], dist="normal"
        )
        
        # q(zt_control | y_control, T_control)
        ZT_control_constr = self.encode_ZT(control_outcomes, control_treatment)
        ZT_control_dist = self.distributionize(
            ZT_control_constr, dim=self.hparams["ZT_dim"], dist="normal"
        )
        
        # q(zxt_control | y_control, T_control, x)
        ZXT_control_constr = self.encode_ZXT(control_outcomes, covariates, control_treatment)
        ZXT_control_dist = self.distributionize(
            ZXT_control_constr, dim=self.hparams["ZXT_dim"], dist="normal"
        )
        
        # q(zx| y, x)
        ZX_constr = self.encode_ZX(outcomes, covariates)
        ZX_dist = self.distributionize(
            ZX_constr, dim=self.hparams["ZX_dim"], dist="normal"
        )
        
        # q(zt| y, T)
        ZT_constr = self.encode_ZT(outcomes, treatments)
        ZT_dist = self.distributionize(
            ZT_constr, dim=self.hparams["ZT_dim"], dist="normal"
        )
        
        # q(zxt| y, T, x)
        ZXT_constr = self.encode_ZXT(outcomes, covariates, treatments)
        ZXT_dist = self.distributionize(
            ZXT_constr, dim=self.hparams["ZXT_dim"], dist="normal"
        )

        # NEW: Causal structure regularizer (Eq.16): L_sim = sim(z_t, z_t^0) - sim(z_x, z_x^0)
        # Use means of variational posteriors as embeddings for stability.
        sim_t = F.cosine_similarity(ZT_constr[..., 0], ZT_control_constr[..., 0], dim=1).mean()
        sim_x = F.cosine_similarity(ZX_constr[..., 0], ZX_control_constr[..., 0], dim=1).mean()
        sim_loss = sim_t - sim_x

        # PRIORS:
        # p(zt_control| T_control) prior
        ZT_control_prior = self.encode_ZT_prior(control_treatment)
        ZT_control_prior_dist = self.distributionize(
            ZT_control_prior, dim=self.hparams["ZT_dim"], dist="normal"
        )
        
        # p(zxt_control| x,T_control) prior
        ZXT_control_prior = self.encode_ZXT_prior(covariates, control_treatment)
        ZXT_control_prior_dist = self.distributionize(
            ZXT_control_prior, dim=self.hparams["ZXT_dim"], dist="normal"
        )
        
        
        # p(zx| x) prior
        ZX_constr_prior = self.encode_ZX_prior(covariates)
        ZX_prior_dist = self.distributionize(
            ZX_constr_prior, dim=self.hparams["ZX_dim"], dist="normal"
        )

        # p(zt| t) prior
        ZT_constr_prior = self.encode_ZT_prior(treatments)
        ZT_prior_dist = self.distributionize(
            ZT_constr_prior, dim=self.hparams["ZT_dim"], dist="normal"
        )
        
        # p(zxt| x,t) prior
        ZXT_constr_prior = self.encode_ZXT_prior(covariates, treatments)
        ZXT_prior_dist = self.distributionize(
            ZXT_constr_prior, dim=self.hparams["ZXT_dim"], dist="normal"
        )
        
        # NOTE: no need of ZX_control_prior, as it the same as ZX_prior
        control_prior_dist = aggregate_normal_distr([ZX_prior_dist.mean, ZXT_control_prior_dist.mean, ZT_control_prior_dist.mean],\
                                                   [ZX_prior_dist.stddev, ZXT_control_prior_dist.stddev, ZT_control_prior_dist.stddev])


        # Sampling the generated (posterior) latent distributions:
        
        ## p(x|ZX)
        ## modified: use ZX_constr[...,0] instead of ZX_constr[0] to correctly get the mus and sigmas (see how MLP.forward reshapes)
        ZX_resample = self.sample_latent(ZX_constr[...,0], ZX_constr[...,1])
        #print(ZX_constr[...,0].shape)
        #print(ZX_constr[...,1].shape)

        # Reconstruct covariates from latent
        ## modified: added "non-sampling" case
        if sample_latent:
            cov_inputs = ZX_resample
        else:
            cov_inputs = ZX_constr[...,0]
        #print("cov_inputs shape:", cov_inputs.shape)
        cov_constr = self.covariate_decode(cov_inputs)

        ## modified: use ZK_k[...,0/1] instead of [0/1] for the same reasons as above
        ZT_resample = self.sample_latent(ZT_constr[...,0], ZT_constr[...,1])
        ZXT_resample = self.sample_latent(ZXT_constr[...,0], ZXT_constr[...,1])
        ZTs = torch.cat([ZT_resample, ZXT_resample], dim=1)
        ZT_control_resample = self.sample_latent(ZT_control_constr[...,0], ZT_control_constr[...,1])
        ZXT_control_resample = self.sample_latent(ZXT_control_constr[...,0], ZXT_control_constr[...,1])
        ZTs_control = torch.cat([ZT_control_resample, ZXT_control_resample], dim=1)

        # Reconstruct treatment from latent
        ## modified: added "non-sampling" case
        if sample_latent:
            treatment_constr = self.intervention_decode(ZTs, ZTs_control)
        else:
            treatment_constr = self.intervention_decode(torch.cat([ZT_constr[...,0], ZXT_constr[...,0]], dim=1), 
                                                        torch.cat([ZT_control_constr[...,0], ZXT_control_constr[...,0]], dim=1))

        # GENERATE (ZX, ZXT, ZT) (POSTERIOR) LATENT DISTRIBUTIONS:

        ## modified: had ZT_control_dist.mean and ZT_control_stddev twice when concatenating the latents
        control_latents_dist_mean = torch.cat([ZX_control_dist.mean, ZXT_control_dist.mean, ZT_control_dist.mean], dim=1)
        control_latents_dist_stddev = torch.cat([ZX_control_dist.stddev, ZXT_control_dist.stddev, ZT_control_dist.stddev], dim=1)
        ## modified: stack means and stddev along last dimension to generate distribution object with .mean and .stddev [batch_size x dimensions]
        control_latents_dist = self.distributionize(
            torch.stack([control_latents_dist_mean, control_latents_dist_stddev], dim=-1),
            dim=control_latents_dist_mean.size(1),
            dist="normal",
        )

        ## modified: added "non-sampling" case
        if sample_latent:
            control_outcomes_constr = self.sample_control(
                control_latents_dist_mean,
                control_latents_dist_stddev,
                size=self.mc_sample_size,
            )
        else:
            control_outcomes_constr = self.forward_control(
                control_latents_dist_mean
            )

        control_outcomes_dist = self.distributionize(control_outcomes_constr)

        exp_latents_dist_mean = torch.cat([ZX_dist.mean, ZXT_dist.mean, ZT_dist.mean], dim=1)
        exp_latents_dist_stddev = torch.cat([ZX_dist.stddev, ZXT_dist.stddev, ZT_dist.stddev], dim=1)
        ## modified: stack means and stddev along last dimension to generate distribution object with .mean and .stddev [batch_size x dimensions]
        exp_dist = self.distributionize(
            torch.stack([exp_latents_dist_mean, exp_latents_dist_stddev], dim=-1),
            dim=exp_latents_dist_mean.size(1),
            dist="normal",
        )
        # print("exp_dist.mean, exp_dist.stddev: ", exp_dist.mean.shape, exp_dist.stddev.shape)

        ## modified: added "non-sampling" case
        if sample_latent:
            expr_outcomes_constr = self.sample_expr(exp_latents_dist_mean, exp_latents_dist_stddev, size=self.mc_sample_size)
        else:
            expr_outcomes_constr = self.forward_expr(exp_latents_dist_mean)

        expr_outcomes_dist = self.distributionize(expr_outcomes_constr)

        # Use discriminator in forward pass to ensure compatibility with DDP
        cov_conditions = torch.cat(covariates, dim=1)
        
        perturb_X = self.permutation_distribution_X(exp_dist.mean, exp_dist.stddev,cov_conditions)
        perturb_T = self.permutation_distribution_T(exp_dist.mean, exp_dist.stddev,treatments)

        permute_T_pred = self.discriminate_T(perturb_T[:, :-1])
        permute_X_pred = self.discriminate_X(perturb_X[:, :-1])

        # COMPUTE LOSS TERMS
        indiv_spec_nllh_control, indiv_spec_nllh_experiments, cov_loss, treatment_loss,\
        kl_divergence_ind, kl_divergence_X, kl_divergence_T, kl_divergence_XT,kl_divergence_control, conditions, conditions_labels = \
        self.loss_paired(control_outcomes, control_outcomes_dist, outcomes, expr_outcomes_dist, exp_dist,
                        ZX_constr, ZT_constr, ZXT_constr, ZX_prior_dist, ZT_prior_dist,ZXT_prior_dist, control_prior_dist, control_latents_dist,
                        cov_constr, treatment_constr, treatments,covariates, 
                        single_treatment=single_treatment)

                
        permute_T_loss = self.loss_discriminator_T(permute_T_pred, perturb_T[:, -1]) 
        permute_X_loss = self.loss_discriminator_X(permute_X_pred, perturb_X[:, -1])
        permute_loss =  0.5 * (permute_T_loss + permute_X_loss)

        # ADDED explicit R2 loss (average gene expression for each cov_pert category)
        cov_names = torch.cat(covariates, -1)
        mean_score = []

        for cov_value in torch.unique(cov_names, dim=0):
            for treat_value in torch.unique(treatments, dim=0):
                cov_mask = (cov_names == cov_value).all(dim=1)
                treat_mask = (treatments == treat_value).all(dim=1)
                idx = torch.where(cov_mask & treat_mask)[0]
                if len(idx) < 2:
                    # print("Warning: condition with less than 2 samples found during R2 computation.")
                    if len(idx) == 0:
                        continue
                # print("Condition with more than 2 samples!")
                expr_yp = expr_outcomes_dist.mean[idx].detach().cpu()
                expr_yp_m = expr_yp.mean(0)
                if torch.any(torch.isnan(expr_yp_m)):
                    print("NaN detected in predicted mean")

                expr_yt = outcomes[idx].detach().cpu()
                expr_yt_m = expr_yt.mean(0)
                if torch.any(torch.isnan(expr_yt_m)):
                    print("NaN detected in true mean")

                r2_expr = r2_score(expr_yt_m, expr_yp_m)

                # Take into account also the associated control sample
                """
                control_yp = control_outcomes_dist.mean[idx].detach().cpu()
                control_yp_m = control_yp.mean(0)
                if torch.any(torch.isnan(control_yp_m)):
                    print("NaN detected in predicted mean for control")

                control_yt = control_outcomes[idx].detach().cpu()
                control_yt_m = control_yt.mean(0)
                if torch.any(torch.isnan(control_yt_m)):
                    print("NaN detected in true mean for control")

                r2_control = r2_score(control_yt_m, control_yp_m)
                """

                # r2_avg = (r2_expr + r2_control)/2
                r2_avg = r2_expr
                mean_score.append(r2_avg)
        
        r2_loss = torch.tensor(np.mean(mean_score), dtype=torch.float64, device=self.device)

        # ORGANIZE LOSS AND LOGGING METRICS

        indiv_spec_nllh = indiv_spec_nllh_control + indiv_spec_nllh_experiments
        # Include causal structure regularizer
        covar_spec_loss = treatment_loss + cov_loss
        kl_divergence_factored = kl_divergence_X + kl_divergence_T + kl_divergence_XT
        kl_divergence_samples= kl_divergence_control + kl_divergence_ind
        kl_divergence = kl_divergence_factored + kl_divergence_samples

        loss = (self.omega0 * indiv_spec_nllh
            + self.omega1 * covar_spec_loss
            + self.omega2 * (kl_divergence_samples + kl_divergence_factored)
            -  self.omega3 * permute_loss   
            +  self.omega4 * sim_loss
            -  self.omega5 * r2_loss
        )

        metrics = {
            "Indiv-spec NLLH": indiv_spec_nllh.item(),
            "Covar-spec Loss": covar_spec_loss.item(),
            "Treatment Loss": treatment_loss.item(),
            "Covariate Loss": cov_loss.item(),
            "Similarity": sim_loss.item(),
            "Treatment Similarity": sim_t.item(),
            "Covariate Similarity": sim_x.item(),
            "KL Divergence": kl_divergence.item(),
            "Discriminator": permute_loss.item(),
            "Permute_T Loss": permute_T_loss.item(),
            "Permute_X Loss": permute_X_loss.item(),
            "R2 Loss": r2_loss.item(),
            } 

        if not adv_training:
            return [loss, metrics]
        
        else:
            return [permute_loss, metrics]
    

    def update_eval_model(self):
        for fcr_module, eval_module in zip(self.fcr_modules, self.eval_modules):
            for param, target_param in zip(
                fcr_module.parameters(), eval_module.parameters()
            ):
                target_param.data.copy_(param.data)
                    

    # Added: increase specified whether the metric should increase or decrease during training
    def early_stopping(self, score, scheduler_autoencoder, scheduler_discriminator, increase=False):
        """
        Decays the learning rate, and possibly early-stops training.
        """
        scheduler_autoencoder.step()
        scheduler_discriminator.step()

        if self.best_score is None:
            self.best_score = -float("inf") if increase else float("inf")

        if (score > self.best_score and increase) or (score < self.best_score and not increase):
            self.best_score = score
            self.patience_trials = 0
        else:
            self.patience_trials += 1

        return self.patience_trials > self.patience

    def init_outcome_emb(self):
        return MLP(
            [self.num_outcomes, self.hparams["outcome_emb_dim"]], final_act="relu"
        )

    def init_treatment_emb(self):
        
        # BUG: the OHE in treatments cannot be used as input for a CompoundEmbedding
        # if self.type_treatments in ("object", "bool", "category", None):
        #     # print(self.num_treatments, self.hparams["treatment_emb_dim"])
        #     return CompoundEmbedding(
        #         self.num_treatments, self.hparams["treatment_emb_dim"]
        #     )
        # else:
        return MLP(
            [self.num_treatments] + [self.hparams["treatment_emb_dim"]] * 2
        )
        
        
    def init_treatment_mixed_emb(self):
        

        return MLP(
                [self.num_treatments] + [self.treatment_mixed_dim] * 2
            )
    

    def init_covariates_emb(self):
        type_covariates = self.type_covariates
        if type_covariates is None or isinstance(type_covariates, str):
            type_covariates = [type_covariates] * len(self.num_covariates)

        covariates_emb = []
        for num_cov, type_cov in zip(self.num_covariates, type_covariates):
            if type_cov in ("object", "bool", "category", None):
                covariates_emb.append(CompoundEmbedding(
                        num_cov, self.hparams["covariate_emb_dim"]
                    ))
            else:
                ## modified: ????
                covariates_emb.append(MLP(
                        [num_cov] + [self.hparams["covariate_emb_dim"]] * 2
                    ))
        return covariates_emb

    # Legacy code: latent_dim is deprecated
    """
    def init_encoder(self):
        return MLP([self.outcome_dim+self.treatment_dim+self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["latent_dim"]],
            heads=2, final_act="relu"
        )
    """
    
   
      
    def init_encoder_exp(self):
        return MLP([self.outcome_dim+self.covariate_dim + self.treatment_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["latent_exp_dim"]],
            heads=2, final_act="relu"
        )
    
    def init_encoder_X(self):
        return MLP([self.outcome_dim+self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZX_dim"]],
            heads=2, final_act="relu"
        )
    
    
    def init_encoder_X_prior(self):
        return MLP([self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZX_dim"]],
            heads=2, final_act="relu"
        )
    
    def init_encoder_T(self):
        return MLP([self.outcome_dim+self.treatment_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZT_dim"]],
            heads=2, final_act="relu"
        )
    
    
    
    def init_encoder_T_prior(self):
        return MLP([self.treatment_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZT_dim"]],
            heads=2, final_act="relu"
        )
    
    ## modified: added the treatment_mixed_dim
    def init_encoder_XT(self):
        return MLP([self.outcome_dim+self.covariate_dim+self.treatment_mixed_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZXT_dim"]],
            heads=2, final_act="relu"
        )
    
    
    def init_encoder_XT_prior(self):
        return MLP([self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["ZXT_dim"]],
            heads=2, final_act="relu"
        )
          
    def init_encoder_control(self):
        return MLP([self.outcome_dim+self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["latent_exp_dim"]],
            heads=2, final_act="relu"
        )

    def init_control_prior(self):
        return MLP([self.covariate_dim]
            + [self.hparams["encoder_width"]] * (self.hparams["encoder_depth"] - 1)
            + [self.hparams["latent_exp_dim"]],
            heads=2, final_act="relu"
        )

    # Legacy code: latent_dim is deprecated
    """
    def init_decoder(self):
        if self.dist_outcomes == "nb":
            heads = 2
        elif self.dist_outcomes == "zinb":
            heads = 3
        elif self.dist_outcomes == "normal":
            heads = 2
        elif self.dist_outcomes == "bernoulli":
            heads = 1
        else:
            raise ValueError("dist_outcomes not recognized")

        return MLP([self.hparams["latent_dim"]+self.treatment_dim]
            + [self.hparams["decoder_width"]] * (self.hparams["decoder_depth"] - 1)
            + [self.num_outcomes],
            heads=heads
        )
    """
    
    def init_decoder_control(self):
        if self.dist_outcomes == "nb":
            heads = 2
        elif self.dist_outcomes == "zinb":
            heads = 3
        elif self.dist_outcomes == "normal":
            heads = 2
        elif self.dist_outcomes == "bernoulli":
            heads = 1
        else:
            raise ValueError("dist_outcomes not recognized")

        # Control decoder consumes the concatenated control latent (ZX+ZXT+ZT)
        # which matches latent_exp_dim rather than latent_dim.
        return MLP([self.hparams["latent_exp_dim"]]
            + [self.hparams["decoder_width"]] * (self.hparams["decoder_depth"] - 1)
            + [self.num_outcomes],
            heads=heads
        )
    
    
    def init_decoder_experiments(self):
        if self.dist_outcomes == "nb":
            heads = 2
        elif self.dist_outcomes == "zinb":
            heads = 3
        elif self.dist_outcomes == "normal":
            heads = 2
        elif self.dist_outcomes == "bernoulli":
            heads = 1
        else:
            raise ValueError("dist_outcomes not recognized")

        return MLP([self.hparams["latent_exp_dim"]]
            + [self.hparams["decoder_width"]] * (self.hparams["decoder_depth"] - 1)
            + [self.num_outcomes],
            heads=heads
        )
    
    def init_decoder_cov(self):
        return MLP([self.hparams["ZX_dim"]]
            + [self.hparams["decoder_width"]] * (self.hparams["decoder_depth"] - 1)
            + [np.max(self.num_covariates)],
            heads=len(self.num_covariates), final_act= "softmax"
        )
    
    
    # NOTE: Duplicate definition of init_decoder_interv existed below; keeping the later one.
    # def init_decoder_interv(self):
    #     return MLP([1]
    #         + [self.num_treatments],
    #         heads=1, final_act= "softmax"
    #     )
    
#     def init_decoder_interv_element(self):
#         return MLP([self.hparams["ZT_dim"]]
#             + [self.num_treatments],
#             heads=1, final_act= "softmax"
#         )
    
#     def init_decoder_interv_concat(self):
#         return MLP([self.hparams["ZT_dim"] + self.hparams["latent_dim"]]
#             + [self.num_treatments],
#             heads=1, final_act= "softmax"
#         )

    def init_discriminator_T(self):
        return MLP([self.hparams["ZT_dim"]+ self.hparams["ZXT_dim"]]
            + [self.hparams["discriminator_width"]] * (self.hparams["discriminator_depth"] - 1)
            + [1]
        )


    def init_decoder_interv(self):
        return MLP([1]
            + [self.treatment_dim],
            heads=1, final_act= "softmax"
        )
    
    def init_decoder_interv_element(self):
        return MLP([self.hparams["ZT_dim"] +self.hparams["ZXT_dim"]]
            + [self.treatment_dim],
            heads=1, final_act= "softmax"
        )
    
    def init_decoder_interv_concat(self):
        return MLP([(self.hparams["ZT_dim"] + self.hparams["ZXT_dim"])*2]
            + [self.treatment_dim],
            heads=1, final_act= "softmax"
        )
    
    
    def init_decoder_interv_single(self):
        return MLP([(self.hparams["ZT_dim"] + self.hparams["ZXT_dim"])*2]
            + [self.treatment_dim],
            heads=1, final_act= "softmax"
        )
    
 
    def init_discriminator_X(self):
        return MLP([self.hparams["ZX_dim"]+ self.hparams["ZXT_dim"]]
            + [self.hparams["discriminator_width"]] * (self.hparams["discriminator_depth"] - 1)
            + [1]
        )

    def move_input(self, input):
        """
        Move minibatch tensors to CPU/GPU.
        """
        if isinstance(input, list):
            return [i.to(self.device) if i is not None else None for i in input]
        else:
            return input.to(self.device)

    def move_inputs(self, *inputs: torch.Tensor):
        """
        Move minibatch tensors to CPU/GPU.
        """
        return [self.move_input(i) if i is not None else None for i in inputs]

    def to_device(self, device):
        self.device = device
        self.to(self.device)

    # BUG: defaults() invalid; please pass hparams explicitly via load_FCR
    # Modified: use ZX, ZT, ZXT encoders instead of exp_encoder, which is not trained
    @torch.no_grad()
    def get_latent(self, outcomes, treatments, covariates):
        
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        ZX_constr = self.encode_ZX(outcomes, covariates)
        ZX_dist = self.distributionize(ZX_constr, dim=self.hparams["ZX_dim"], dist="normal")

        ZXT_constr = self.encode_ZXT(outcomes, covariates, treatments)
        ZXT_dist = self.distributionize(ZXT_constr, dim=self.hparams["ZXT_dim"], dist="normal")

        ZT_constr = self.encode_ZT(outcomes, treatments)
        ZT_dist = self.distributionize(ZT_constr, dim=self.hparams["ZT_dim"], dist="normal")
        
        ZX_resample = self.sample_latent(ZX_dist.mean, ZX_dist.stddev)
        ZXT_resample = self.sample_latent(ZXT_dist.mean, ZXT_dist.stddev)
        ZT_resample = self.sample_latent(ZT_dist.mean, ZT_dist.stddev)
        return ZX_resample,ZXT_resample, ZT_resample
    
    @torch.no_grad()
    def get_latent_means(self, outcomes, treatments, covariates):
        """
        Deterministic latent representations (posterior means).

        Returns the means of q(zx|y,x), q(zxt|y,t,x), q(zt|y,t) without sampling.
        Useful for visualization (e.g., UMAP) to avoid adding sampling noise.
        """
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        ZX_constr = self.encode_ZX(outcomes, covariates)
        ZX_dist = self.distributionize(ZX_constr, dim=self.hparams["ZX_dim"], dist="normal")

        ZXT_constr = self.encode_ZXT(outcomes, covariates, treatments)
        ZXT_dist = self.distributionize(ZXT_constr, dim=self.hparams["ZXT_dim"], dist="normal")

        ZT_constr = self.encode_ZT(outcomes, treatments)
        ZT_dist = self.distributionize(ZT_constr, dim=self.hparams["ZT_dim"], dist="normal")

        ZX_mean = ZX_dist.mean
        ZXT_mean = ZXT_dist.mean
        ZT_mean = ZT_dist.mean

        return ZX_mean, ZXT_mean, ZT_mean
    
    @torch.no_grad()
    def get_latent_presentation(self, outcomes, treatments, covariates, sample=False):
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        if not sample:
            # Return posterior means (deterministic) for visualization
            ZX_mean, ZXT_mean, ZT_mean = self.get_latent_means(
                outcomes, treatments, covariates
            )
            return ZX_mean, ZXT_mean, ZT_mean
        else:
            # FIXED BY MARINA - Proper sampling without double resampling
            # FIXED BY RAFA - Use trained encoders instead of exp_encoder
            ZX_sample, ZXT_sample, ZT_sample = self.get_latent(
                outcomes, treatments, covariates
            )
            return ZX_sample, ZXT_sample, ZT_sample
    
    
    @torch.no_grad()
    def get_latent_prior_presentation(self, outcomes, treatments, covariates):
        outcomes, treatments, covariates = self.move_inputs(
            outcomes, treatments, covariates
        )
        ZX_constr_prior = self.encode_ZX_prior(covariates)
        ZX_prior_dist = self.distributionize(
            ZX_constr_prior, dim=self.hparams["ZX_dim"], dist="normal"
        )

        ## p(z_t|t) prior
        ZT_constr_prior = self.encode_ZT_prior(treatments)
        ZT_prior_dist = self.distributionize(
            ZT_constr_prior, dim=self.hparams["ZT_dim"], dist="normal"
        )
        
        ## p(z_xt|x,t) prior
        ZXT_constr_prior = self.encode_ZXT_prior(covariates, treatments)
        ZXT_prior_dist = self.distributionize(
            ZXT_constr_prior, dim=self.hparams["ZXT_dim"], dist="normal"
        )
        ZX_resample = self.sample_latent(ZX_prior_dist.mean, ZX_prior_dist.stddev)
        ZT_resample = self.sample_latent(ZT_prior_dist.mean, ZT_prior_dist.stddev)
        ZXT_resample = self.sample_latent(ZXT_prior_dist.mean, ZXT_prior_dist.stddev)
        
        return ZX_resample, ZT_resample, ZXT_resample
        
        

    @torch.no_grad()
    def get_control_latent(self, outcomes, covariates):
        
        control_latent_constr = self.encode_control(outcomes, covariates)
        control_latents_dist = self.distributionize(
            control_latent_constr, dim=self.hparams["latent_exp_dim"], dist="normal"
        )
        return control_latents_dist.resample()
    
    
    @torch.no_grad()
    def get_control_representation(self, outcomes, covariates):
        outcomes, covariates = self.move_inputs(
            outcomes, covariates
        )
        latents = self.get_control_latent(
            outcomes, covariates
        )
        return latents

    # ADDED BY RAFA: Activate / Deactivate discriminator
    def freeze_discriminator(self, freeze=True):
        if freeze:
            for p in self.discriminator_T.parameters():
                p.requires_grad_(False)
            for p in self.discriminator_X.parameters():
                p.requires_grad_(False)
        else:
            for p in self.discriminator_T.parameters():
                p.requires_grad_(True)
            for p in self.discriminator_X.parameters():
                p.requires_grad_(True)
